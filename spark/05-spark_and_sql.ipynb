{"cells":[{"cell_type":"markdown","metadata":{"id":"hI-VWQK9sYZ2"},"source":["# Spark DataFrames and Spark SQL "]},{"cell_type":"markdown","metadata":{"id":"p8ovQi40sYZ3"},"source":["#### <font color=\"red\">The below cell is only for setting spark environment on google colabs (no need to run this if your are running spark locally or on a cluster) </font>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"2OzYd26jsYZ3","executionInfo":{"status":"error","timestamp":1671106414117,"user_tz":-60,"elapsed":109053,"user":{"displayName":"Alejandro AO","userId":"08566640190962735867"}},"outputId":"a0fd4005-e156-49a0-eb49-4dd3d63fbc4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["^C\n"]},{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-676288ccbbe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# mount your google drive to be able to access files from your google drive !\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    122\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    125\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["# for installing java\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# for downloading hadoop for spark\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n","\n","# unzipping hadoop for spark\n","!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n","\n","# setting environment variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n","\n","# installing spark on the current environment\n","!pip install -q findspark\n","import findspark\n","findspark.init()\n","\n","# mount your google drive to be able to access files from your google drive ! \n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"0Y6MFC7AsYZ4"},"source":["First let us always import pyspark and create a <b>SparkSession</b> from pyspark.sql for working with DataFrame.\n","\n","SparkSession for DataFrames plays the same role as SparkContext for RDDs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEhGm-cnsYZ4"},"outputs":[],"source":["import pyspark\n","\n","spark = pyspark.sql.SparkSession.builder.appName(\"Spark-Dataframe-SQL\").getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"rghPFafqsYZ4"},"source":["##  <font color=\"blue\"> 1 - Why Spark Dataframes ? </font>\n","\n","\n","We saw in our previous chapter on shuffling with RDD that it is common to have multiple solutions to solve the same issue with RDD, but with different performances (e.g. reduceByKey vs groupByKey or filtering data before joins). It is for the programmer to pay attention at choose the right solution.\n","\n","If we take for instance the last example with filtering and joins, we present two solutions with different performances:\n","\n","<b> <i> 1) kv_freq_stations.join(kv_subscriptions)\n","                .filter(lambda tuple:tuple[1][0]=='Capitole' or tuple[1][0]=='Compans')\n","                .count()\n","</i></b>              \n","          \n","\n","<img src='https://drive.google.com/uc?id=1MiZOuM64aAh_mbnSu0psMsY3PkNTPXlw' />\n","\n","<b> <i>                \n","2) kv_freq_stations.filter(lambda id_station:id_station[1]=='Capitole' or id_station[1]=='Compans')\n","                .join(kv_subscriptions)\n","                .count()\n","</i> </b>  \n","\n"," <img src='https://drive.google.com/uc?id=1VFUJS8OPHHO2UFgK3bK7XRVT1y-HG2R8' />\n","\n","               \n","The second solution is better because we minimize data shuffling that will happen with the join transformation, by applying first the filter transformation.\n","\n","<b>But it would be nice if Spark automatically knew, if we wrote code with the fisrt solution, that it could automatically rewrite our code as in solution 2 before execution ! </b>\n","    \n","<font color=\"green\"> This is for example where Spark Dataframes is interesting, but at one condition :  </font> <font color=\"orange\"> you should provide a bit of extra structural information so that Spark can do many optimizations for you!  </font>\n","\n","<br/>\n","\n","Spark Dataframes are designed to work with <b>structured data</b>. Structured data are data with a fixed schema (a set of columns with a fixed datatype for each column). Structured data are commonly used in relational databases (e.g. Oracle database, MySQL, PostgreSQL) or with formats such as excel files or csv files. With structured data, Spark have an extra information on the content of data, a Spark dataframe can be seen as a relational database table (with the difference that these dataframes are distributed over a cluster of machines). Thus with Dataframes, before executing transformations or queries (with a well known set of SQL operators) Spark can apply many optimizations plans that exists since decades from the relational database community, and can send data over the network in a smarter way by selecting and serializing only useful parts to send. The below figure show a big view of the difference between an RDD and a Dataframe with the customers frequent stations of our last example.\n","\n","<img src='https://drive.google.com/uc?id=1JQ7nTw_CCy-ewkA_8zfkg4j2DOiaUiCr' />\n","\n","\n","Another very important difference between RDD and Dataframe is that with RDD, we can only use functional transformations with functions (map, filter, reduce, etc.) for analyzing our data. However, because Spark Dataframe are similar to relational databases tables, Spark provides a structured language <b>(Spark SQL)</b> very similar to SQL and that can be directly used to query and analyze distributed data in Spark Dataframes. This is very important because many data analysts are familiar with SQL, and they can easily reuse their SQL knowledge to analyze distributed data with Spark Dataframes and Spark SQL!\n","    \n","A very simple rule of using RDD vs Dataframe can be:\n","\n","- RDD can be used mostly for processing unstructured data: data without any structure such as text analysis (e.g. comments from social medias, tweets, texts from log files, etc.)\n","- Dataframes can be used mostly for processing structured data: data with well known columns and datatypes usually extracted from csv files, excel files, or relational databases. It can also be used for processing semi-structured data: data with partial structure that can be extracted from file formats such as XML or JSON.\n","\n","    \n","##  <font color=\"blue\"> 2 - Spark SQL  stack </font>\n","\n","SQL is a reference language for doing analytics. Spark SQL is the spark interface for distributed processing using SQL syntax on top of Dataframes and RDD. It has three main goals:\n","\n","- Support <b> relational processing </b> bith within Spark programs (on RDDs) and on external data sources with friendly API: sometimes it's more desirable to express a computation in SQL syntax than with functional APIs and vice versa.\n","- High performance, achieved by using optimization techniques from research in databases.\n","- Easily support new data sources such as semi-structured data and external databases.\n","\n","Spark SQL is a component of the Spark stack as we can see in the below figure.\n","\n","<img src='https://drive.google.com/uc?id=1Fvx5HbKTdfqUDy1fNjhDfcZL43tk77Ea' />\n","\n","\n","Spark SQL uses DataFrame as support for applying SQL queries. DataFrames contains structured data with a schema and are built on top of Spark RDD. That means, at the end DataFrames are stored in the clusters as RDD ! But with the additional information on DataFrames content (the schema) Spark uses a dedicated optimizer (Catalyst) to optimize their operators before reaching (transparently) the RDD layer. As we will see next in this chapter, we can analyze Spark DataFrames in two ways:\n","\n","- with SQL syntax (Spark SQL)\n","- with DataFrame API functions\n","\n","Spark SQL and DataFrame API can be used directly from the Spark Console or User Programs (python in this course).\n","\n","JDBC is just a connector for that can be used if one want one want to get data from external databases into Spark.\n","\n","As Spark RDD, Spark DataFrames are <b> immutables </b>, <b>lazily evaluated</b>, and <b>distributed</b>\n","\n","Before starting using Spark SQL, we will first see how to create Spark DataFrames.\n","\n","##  <font color=\"blue\"> 3 - Creating Spark DataFrames </font>\n","\n","Spark DataFrames can be created from multiple data sources such as Spark RDDs, structured or semi-structured data files (e.g. csv, json, xml), SQL Databases or NoSQL Databases (see below figure). For each database, they usually provide a specific connector implemented via JDBC. In this section we will look at the first two options (from RDDs and data files).\n","\n","<img src='https://drive.google.com/uc?id=13v2sYSf8E_gY3MsRYCirSC1v9umH7IVc' />\n","\n","\n","### <font color=\"green\"> 3.1 Creating a DataFrame from an existing RDD </font>\n","\n","We know that a DataFrame needs a structure with a schema. For creating a DataFrame from an RDD with can do it in two ways: with a schema reflectively inferred or with a schema explicitly specified.\n","\n","#### <font color=\"red\"> 3.1.1 with a reflectively inferred schema </font>\n","\n","In this case we just need to call <b><i>toDF</i></b> method on the RDD as in the following examples:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkKvyVrPsYZ5"},"outputs":[],"source":["# we create our previous purchase RDD example\n","\n","list_purchases = [(100, 'Toulouse', 'Limoges', 22.25),(100, 'Toulouse','Paris', 31.60), (100, 'Paris', 'Orleans', 12.40), (200, 'Toulouse', 'Muret', 8.20), (300, 'Marseille','Paris', 42.10), (300, 'Arles', 'Nimes', 16.20)] \n","\n","# we can access a SparContext from SparkSession with spark.SparkContext\n","kv_rdd_purchases = spark.sparkContext.parallelize(list_purchases)\n","\n","kv_rdd_purchases.take(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etRnFUQZsYZ6"},"outputs":[],"source":["# to create the dataframe from the RDD we can specify a list of columns names to the toRDD method\n","\n","purchases_df = kv_rdd_purchases.toDF(['customer_id','origin','destination','price'])\n","\n","# for visualizing a sample of rows of our dataframe we can use the show method (by default prints 20 rows)\n","purchases_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YRhz3UgXsYZ6"},"outputs":[],"source":["# What happens if we don't specify the columns names to the toDF method ?\n","\n","purchases_df2 = kv_rdd_purchases.toDF()\n","\n","purchases_df2.show()"]},{"cell_type":"markdown","metadata":{"id":"3b2Yz2llsYZ6"},"source":["We can see that if we don't specify column names, spark automatically assign columns names in the form: _1, _2, _3, ... (not easy for readability). It is always a good idea to specify explicit column names."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"St0u02BFsYZ6"},"outputs":[],"source":["# Let's print the inferred schema of our purchases_df DataFrame with printSchema() method \n","\n","purchases_df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"eKTjbBsMsYZ6"},"source":["We can see that by inferring the schema, spark infer a data type for each column (depending on the values on that column),\n","and also automatically set them to be nullable (they can contain no value).\n","\n","Sometimes we will want to specify explicitly the schema...\n","\n","#### <font color=\"red\"> 3.1.2 with an explicit schema </font>\n","\n","In this case, we need to <b>specify the schema and use the <i>createDataFrame</i> method of our SparkSession</b>.\n","\n","In his simplest form a schema is a StructType with a list of StructField (a StructField represents a column), and each StrucField (column) need 3 information (the name of the column, the data type, and a boolean flag indicating if the column can be nullable or not).\n","\n","For instance, for our previous purchase RDD, we may want a schema like this:\n","\n","<i>purchaseSchema = StructType ( [StructField(\"customer_id\", IntegerType(), False), <br/>\n","                               StructField(\"origin\", StringType(), False),<br/>\n","                               StructField(\"destination\", StringType(), False),<br/>\n","                               StructField(\"price\", DoubleType(), False)] ) <br/> </i>\n","\n","In this explicit schema (compared to the implicit one) we want our customer_id to be of type Integer (it was implicitly long), and all our columns to be not nullable for instance (They was implicitly nullable).\n","\n","Below, we create this schema, and use it explicitly to create our purchase dataframe from the RDD by using the createDataFrame\n","method of our SparkSession:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWUr70OGsYZ6"},"outputs":[],"source":["# we first need to import types (e.g. StructType, StructField, IntegerType, etc.)\n","from pyspark.sql.types import *\n","\n","# schema creation\n","\n","purchaseSchema =StructType ( [StructField(\"customer_id\", IntegerType(), False),\n","                              StructField(\"origin\", StringType(), False),\n","                              StructField(\"destination\", StringType(), False),\n","                              StructField(\"price\", DoubleType(), False)] )\n","\n","# DataFrame creation\n","purchase_df=spark.createDataFrame(kv_rdd_purchases, purchaseSchema)\n","\n","# let us show some sample rows\n","purchase_df.show()"]},{"cell_type":"markdown","metadata":{"id":"iA4tnjG9sYZ7"},"source":["Let's verify that we have now our explicit schema"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f47CXlkKsYZ7"},"outputs":[],"source":["purchase_df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"93UgSu8dsYZ7"},"source":["### <font color=\"green\"> 3.2 Creating a DataFrame from a data file </font>\n","\n","In most cases DataFrames are created from structured or semi-structured data files (e.g. csv, json, xml) stored for instance on HDFS, cloud storages (e.g. Amazon S3, MS Azure Storage),  or local file systems (e.g. windows, unix).\n","\n","The methods for reading each data file format are quite similar (see documentation: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html). \n","\n","For instance for read csv files, we can call the <b>spark.read.csv</b> method (spark is the SparkSession) as in the below example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVeO-eSrsYZ7"},"outputs":[],"source":["# our previous purchases sample data are stored here in a csv files\n","# if the first row of file contains columns header, we should specify the parameter header=True\n","# many other parameters could be specified (e.g. separator, schema, null values, ...) see the documentation\n","\n","purchase_df = spark.read.csv('drive/MyDrive/PYSPARK-COURSE/purchases_sample.csv', header=True)\n","\n","purchase_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfgTT_KEsYZ7"},"outputs":[],"source":["purchase_df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"gqS-B-B_sYZ7"},"source":["We read the csv file without a specific schema, you can notice that for instance the type of customerId is inferred by default with String type!\n","\n","If we want to read a file with a specific schema, we can set the schema parameter:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOtuyf2isYZ7"},"outputs":[],"source":["purchase_df = spark.read.csv('drive/MyDrive/PYSPARK-COURSE/purchases_sample.csv', schema=purchaseSchema, header=True)\n","\n","purchase_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjQEZd1PsYZ7"},"outputs":[],"source":["purchase_df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"NI-etm08sYZ7"},"source":["now, the customerId is renamed customer_id and is of type String as specified in the previous <i>purchaseSchema</i>"]},{"cell_type":"markdown","metadata":{"id":"gRY5fJa0sYZ7"},"source":["##  <font color=\"blue\"> 4 - Spark SQL queries on DataFrames </font>\n","\n","now how to query or analyze DataFrames with Spark SQL ? The answer is quite simple:\n","    \n","- first we need to register the DataFrame as a temporary view (a little similar to a table or view of a database)\n","- second we can apply many common SQL queries on this view\n","\n","###  <font color=\"green\"> 4.1 - Creating the temporary view from the DataFrame </font>\n","\n","We just need to call the method <i>createOrReplaceTempView</i> on our DataFrame and specify the name of our view\n","(that will be used later for queries)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5cMizl0sYZ7"},"outputs":[],"source":["# we can also use the method createTempView if we don't want to replace an already existing view with the same name\n","\n","purchase_df.createOrReplaceTempView('purchase')"]},{"cell_type":"markdown","metadata":{"id":"Buw926W0sYZ7"},"source":["### <font color=\"green\"> 4.2 - Spark SQL queries </font>\n","\n","The SQL queries available in Spark SQL includes many standard SQL statements such as:\n","    \n","- SELECT\n","- FROM\n","- WHERE\n","- GROUP BY\n","- ORDER BY\n","- HAVING\n","- COUNT\n","- DISTINCT\n","- JOIN\n","- (LEFT|RIGHT|FULL) JOINS\n","- Subqueries ...\n","- ...\n","\n","It is also, largely what is available in Hive (HiveQL): <i>Spark SQL is very similar to Hive</i>\n","    \n","The full supported syntax of SPARK SQL is provided here:https://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/spark/sparkSqlSupportedSyntax.html\n","\n","For executing our query we can just the <b>spark.sql</b> method (spark is the SparkSession) by specifying our query as parameter.\n","\n","<i>It is also important to remember that Spark SQL is not a relational database engine ! (see again the Spark SQL stack figure). When we execute a query on a DataFrame in Spark SQL, this query transparently optimized (with Catalyst) before transparently transformed under the cover into rdd api transformations and actions (that operate on distributed data in the cluster) !</i>\n","\n","Below are some query examples in Spark SQL:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8rFChVVMsYZ8"},"outputs":[],"source":["# e.g. here we want to get (select) all the rows and columns of our dataframe\n","# remember: we register our purchase_df into the purchase view in the previous section\n","\n","purchase_all_df = spark.sql(\" SELECT * FROM purchase \")\n","\n","purchase_all_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUGVmu8TsYZ8"},"outputs":[],"source":["# e.g. here we want to get (select) all the rows but only the customer_id and price columns of our dataframe\n","\n","purchase_id_price_df = spark.sql(\" SELECT customer_id,price FROM purchase \")\n","\n","purchase_id_price_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNkNSLwfsYZ8"},"outputs":[],"source":["# e.g. here we want to get (select) only the customer_id and price columns and only rows with prices greater than 20 euros\n","\n","purchase_id_price_gt20_df = spark.sql(\" SELECT customer_id,price FROM purchase WHERE price > 20 \")\n","\n","purchase_id_price_gt20_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbAn-eSusYZ8"},"outputs":[],"source":["# e.g. as previous, but with prices in ascending order\n","\n","purchase_id_price_gt20_asc_df = spark.sql(\" SELECT customer_id,price FROM purchase WHERE price > 20 ORDER BY price asc \")\n","\n","purchase_id_price_gt20_asc_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tx_TWdysYZ8"},"outputs":[],"source":["# e.g. as previous, but with prices in descending order\n","\n","purchase_id_price_gt20_desc_df = spark.sql(\" SELECT customer_id,price FROM purchase WHERE price > 20 ORDER BY price desc \")\n","\n","purchase_id_price_gt20_desc_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzvvpa-DsYZ8"},"outputs":[],"source":["# e.g. here we want to get (select) all columns columns and only rows where origin is Paris or Marseille \n","\n","purchase_paris_marseille_df = spark.sql(\" SELECT * FROM purchase WHERE origin='Paris' or origin='Marseille' \")\n","\n","purchase_paris_marseille_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ee0Ln1_1sYZ8"},"outputs":[],"source":["# e.g. here we want to get (select) distinct origin stations\n","\n","purchase_distinct_origin_df = spark.sql(\" SELECT DISTINCT origin FROM purchase \")\n","\n","purchase_distinct_origin_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDgj334XsYZ8"},"outputs":[],"source":["# e.g. here we want to count the total number of rows in our dataframe\n","\n","purchase_count_df = spark.sql(\" SELECT COUNT(*) FROM purchase \")\n","\n","purchase_count_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VyjBPar-sYZ8"},"outputs":[],"source":["# e.g. here we want to count the total number of rows from Toulouse origin in our dataframe\n","\n","purchase_origin_toulouse_count_df = spark.sql(\" SELECT COUNT(*) FROM purchase WHERE origin='Toulouse' \")\n","\n","purchase_origin_toulouse_count_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cw6UjxuksYZ8"},"outputs":[],"source":["# e.g. here we want to get (select) the average price per customer \n","# remember that we use groupByKey or reduceByKey for this same question when using RDD in the previous chapter\n","# as we have structured data in this case, the following solution with DataFrame is will even be more optimized !\n","\n","purchase_avg_df = spark.sql(\" SELECT customer_id,avg(price) FROM purchase GROUP BY customer_id \")\n","\n","purchase_avg_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMWP9eOCsYZ8"},"outputs":[],"source":["# e.g. as previously but we want to rename the avg (price) column to mean_price (we use the \"as\"...)\n","\n","purchase_avg_df = spark.sql(\" SELECT customer_id, avg(price) as mean_price  FROM purchase GROUP BY customer_id \")\n","\n","purchase_avg_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_5ykH6zsYZ8"},"outputs":[],"source":["# e.g. here we want to get (select) total prices per customer for purchases from Toulouse origin\n","\n","purchase_origin_toulouse_total_df = spark.sql(\" SELECT customer_id, sum(price) as total_price \\\n","                                                FROM purchase \\\n","                                                WHERE origin='Toulouse'\\\n","                                                GROUP BY customer_id \")\n","\n","purchase_origin_toulouse_total_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhhPn-SpsYZ9"},"outputs":[],"source":["# e.g. as previous we still want to get (select) total prices per customer for purchases from Toulouse origin, but also with \n","# only a total prices greater than 50 euros\n","\n","\n","purchase_origin_toulouse_total_gt_50_df = spark.sql(\" SELECT customer_id, sum(price) as total_price \\\n","                                                      FROM purchase \\\n","                                                      WHERE origin='Toulouse'\\\n","                                                      GROUP BY customer_id \\\n","                                                      HAVING total_price > 50\")\n","\n","purchase_origin_toulouse_total_gt_50_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKAdMFGPsYZ9"},"outputs":[],"source":["# e.g. we want the number of journeys from each origin station for each customer, order by origin station (ascending) and number of journeys\n","# (descending)\n","\n","purchase_origin_customer_nb_df = spark.sql(\" SELECT origin, customer_id, count(*) as nb_journeys \\\n","                                             FROM purchase \\\n","                                             GROUP BY origin, customer_id \\\n","                                             ORDER BY origin asc, nb_journeys desc\")\n","\n","purchase_origin_customer_nb_df.show()"]},{"cell_type":"markdown","metadata":{"id":"YaWBdzidsYZ9"},"source":["For join examples, we use here sample data from our previous frequent stations and subscription example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBXJvHH9sYZ9"},"outputs":[],"source":["# we read the frequent stations csv file \n","\n","frequent_stations_df = spark.read.csv('drive/MyDrive/PYSPARK-COURSE/frequent_stations.csv', header=True)\n","\n","frequent_stations_df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xn2VCS8YsYZ9"},"outputs":[],"source":["# we read the subscriptions csv file\n","\n","subscriptions_df = spark.read.csv('drive/MyDrive/PYSPARK-COURSE/subscriptions.csv', header=True)\n","\n","subscriptions_df.show()"]},{"cell_type":"markdown","metadata":{"id":"xV1XtgPgsYZ9"},"source":["Before applying queries, we register the two dataframes as temporary view"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZ8_d1j7sYZ9"},"outputs":[],"source":["frequent_stations_df.createOrReplaceTempView('stations')\n","\n","subscriptions_df.createOrReplaceTempView('subscriptions')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eo-k716WsYZ9"},"outputs":[],"source":["# e.g. we want all informations on customer frequent stations with their corresponding subscription information\n","\n","subcriptions_frequent_stations_join_df = spark.sql(\" SELECT * \\\n","                                                    FROM stations \\\n","                                                    JOIN subscriptions \\\n","                                                    ON stations.customer_id = subscriptions.customer_id \")\n","\n","subcriptions_frequent_stations_join_df.show()"]},{"cell_type":"markdown","metadata":{"id":"XcG48nTFsYZ9"},"source":["As you see, we perform  a SQL inner join between frequent stations and customer subscriptions. Note that unlike key/value RDD, there is no key/value notion in DataFrame, we have to specify which columns (or conditions) we should join on. Here the join condition is to retain only rows with the same customer_id in both views. Thus, the customers with ids 104 and 999 are not present in the result."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"87-Ae5zssYZ9"},"outputs":[],"source":["# e.g. we want all informations on customers subcriptions with or without their corresponding  frequent stations information\n","\n","subscriptions_left_join_frequent_stations_df = spark.sql(\" SELECT * \\\n","                                                           FROM subscriptions \\\n","                                                           LEFT JOIN stations \\\n","                                                           ON stations.customer_id = subscriptions.customer_id \")\n","\n","subscriptions_left_join_frequent_stations_df.show()"]},{"cell_type":"markdown","metadata":{"id":"dRjixC_8sYZ9"},"source":["As you see, by using the left join in this case, customer with id 104 also appear in the result with no information (null) \n","for his frequent stations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyhAGsAPsYZ-"},"outputs":[],"source":["# e.g. we want all informations on customers frequent stations with or without their corresponding subcriptions information\n","\n","subscriptions_right_join_frequent_stations_df = spark.sql(\" SELECT * \\\n","                                                            FROM subscriptions \\\n","                                                            RIGHT JOIN stations \\\n","                                                            ON stations.customer_id = subscriptions.customer_id \")\n","\n","subscriptions_right_join_frequent_stations_df.show()"]},{"cell_type":"markdown","metadata":{"id":"q5OhubfvsYZ-"},"source":["As you see, by using the right join in this case, customer with id 999 also appear in the result with no information (null) for his subscription"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DE0BS4N1sYZ-"},"outputs":[],"source":["# count the total  number of journeys of registered customers that frequently travel to Capitole or Compans stations ?\n","# remember this was our example at the beginning of the chapter\n","# By using RDDs, the performance of writing filter before or after the join will have a great impact on performance\n","# As as have structured data, the following solution with DataFrame and Spark SQL will be automatically optimized by Spark ! (by Catalyst component)\n","\n","nb_journeys_frequent_capitole_compans_df = spark.sql(\" SELECT count(*) as nb_journeys \\\n","                                                       FROM subscriptions \\\n","                                                       JOIN stations \\\n","                                                       ON stations.customer_id = subscriptions.customer_id \\\n","                                                       WHERE stations.frequent_station=='Capitole' or stations.frequent_station=='Compans' \")\n","\n","nb_journeys_frequent_capitole_compans_df.show()"]},{"cell_type":"markdown","metadata":{"id":"J6ifL9iTsYZ-"},"source":["<i>We use a sample small dataset to show some examples of how to use Spark SQL. But these processing steps will be the same for analyzing Terabytes or Petabytes of structured data in a cluster with hundreds or thousand nodes for instance ! </i>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}